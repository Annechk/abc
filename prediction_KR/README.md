# LSTM для прогнозирования цены акций

## 1. ПОЛНОЕ ЗАДАНИЕ ИЗ МЕТОДИЧКИ

### Вариант 3: Time Series Prediction (Прогнозирование временных рядов)

**Задача:** Создать LSTM сеть для прогнозирования цены акций.

**Требования:**
- **Язык программирования:** Python 3.7+
- **Фреймворк:** TensorFlow/Keras
- **Платформа:** Google Colab

**Обязательные параметры архитектуры:**
- Использовать скользящие окна размером 60 дней для подготовки данных
- LSTM слой с 50 нейронами (first LSTM layer)
- Dropout слой (0.2)
- LSTM слой с 50 нейронами (second LSTM layer)  
- Dropout слой (0.2)
- Dense слой с 25 нейронами
- Выходной слой (1 нейрон)

**Параметры обучения:**
- Нормализация данных: Min-Max scaling в диапазон [0, 1]
- Количество эпох: 50
- Размер батча: 32
- Разделение данных: 80% обучение, 20% валидация
- Оптимизатор: Adam (learning rate = 0.001)
- Функция потерь: MSE (Mean Squared Error)

**Требуемые результаты:**

1. **Визуализация обучения:** 2 графика
   - График 1: Точность обучения и валидации (возрастающая линия)
   - График 2: Функция потерь обучения и валидации (убывающая линия)
   - Обе линии должны быть на одном графике (синяя и оранжевая)

2. **Прогнозирование:**
   - Предсказание цены на 30 дней вперед
   - Визуализация прогноза на графике с историческими данными

3. **Оценка качества модели:**
   - MSE (Mean Squared Error)
   - RMSE (Root Mean Squared Error)
   - MAE (Mean Absolute Error)
   - R² Score

4. **Код:**
   - Модульная архитектура с классом TimeSeriesPredictor
   - Методы: build_model(), normalize(), denormalize(), prepare_data(), train(), predict(), make_predictions(), plot_training_history()

---

## 2. АЛГОРИТМ РАБОТЫ НС ПО БЛОКАМ

### Блок 1: Подготовка данных

#### ШАГ 1.1: Генерирование цен
```python
trend = np.linspace(100, 150, 500)    # Тренд от 100 до 150
noise = np.random.normal(0, 2, 500)   # Случайный шум
prices = trend + noise                # Комбинируем
```
**Зачем:** Создаём 500 дневных цен с восходящим трендом.

---

#### ШАГ 1.2: Нормализация
```python
def normalize(self, data):
    return self.scaler.fit_transform(data.reshape(-1, 1)).flatten()
# Было: [50, 102, 103, ..., 149]
# Стало: [0, 0.35, 0.36, ..., 0.99]
```
**Зачем:** LSTM быстрее обучается на нормализованных [0,1] данных. Избегаем взрыва градиентов.

**Формула:** `x_норм = (x - x_min) / (x_max - x_min)`

---

#### ШАГ 1.3: Скользящие окна
```python
for i in range(len(data) - 60):
    window = data[i:i + 60]      # 60 дней истории
    y = data[i + 60]              # Следующий день
    X.append(window)
# Результат: 440 образцов (500 - 60)
```
**Зачем:** 
- LSTM работает с последовательностями, не с одиночными числами
- Каждый образец: 60 дней → предсказание 1 дня
- Создаём 440 обучающих примеров из 500 цен!

**Пример:**
```
Образец 1: [цена дня 1-60] → цена дня 61
Образец 2: [цена дня 2-61] → цена дня 62
...
Образец 440: [цена дня 441-500] → цена дня 501
```

---

#### ШАГ 1.4: Разделение на обучение/валидацию
```python
split_idx = int(len(X) * 0.8)      # 352 образца
X_train, X_val = X[:352], X[352:]  # 352 + 88
y_train, y_val = y[:352], y[352:]
```
**Зачем:** 80% на обучение, 20% для проверки качества на новых данных.

---

### Блок 2: Построение архитектуры LSTM

```python
self.model = keras.Sequential([
    layers.LSTM(50, activation='relu', return_sequences=True, 
               input_shape=(60, 1)),
    # LSTM слой: 50 нейронов, 60 временных шагов
    # return_sequences=True: выдаём все 60 шагов (не только последний)
    
    layers.Dropout(0.2),
    # Отключаем 20% нейронов → предотвращаем переобучение
    
    layers.LSTM(50, activation='relu', return_sequences=False),
    # Второй LSTM: обучает сложные зависимости
    # return_sequences=False: выдаём только финальный выход
    
    layers.Dropout(0.2),
    
    layers.Dense(25, activation='relu'),
    # Dense: трансформирует 50 чисел в 25 чисел
    
    layers.Dense(1)
    # Выход: 1 число (предсказанная цена)
])
```

**Архитектура:**
```
Вход (60, 1) → LSTM(50) → Dropout → LSTM(50) → Dropout → Dense(25) → Dense(1) → Выход
```

---

### Блок 3: Обучение модели

```python
self.history = self.model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=50,        # 50 проходов по всем 352 образцам
    batch_size=32,    # Обновляем веса на 32 образцах за раз
    verbose=1
)
```

**Что происходит в каждой эпохе:**

1. **FORWARD PASS** → вычисляем выходы: `y_pred = model(X_train)`
2. **COMPUTE LOSS** → ошибка: `loss = (y_true - y_pred)²`
3. **BACKWARD PASS** → вычисляем градиенты для каждого веса
4. **UPDATE WEIGHTS** → `W = W - learning_rate × gradient`

**Результат:** Loss падает, Accuracy растёт через 50 эпох.

---

### Блок 4: Прогнозирование на 30 дней

```python
current_window = normalized_data[-60:]  # Последние 60 дней

for step in range(30):
    pred = self.predict(current_window)      # Предсказываем
    predictions.append(pred)
    # Сдвигаем окно: убираем первый день, добавляем предсказание
    current_window = np.append(current_window[1:], pred)
```

**Процесс:**
```
День 501: окно [дни 441-500] → модель → прогноз 0.85
День 502: окно [дни 442-501] → модель → прогноз 0.86
День 503: окно [дни 443-502] → модель → прогноз 0.87
... (30 раз)

Результат: 30 нормализованных предсказаний
```

**Денормализация:** `predictions_denorm = scaler.inverse_transform(predictions)`
```
Было: [0.85, 0.86, 0.87]
Стало: [145$, 146.5$, 147$]
```

---

### Блок 5: Оценка качества

```python
predictions_val = model.predict(X_val)  # 88 предсказаний

mse = mean_squared_error(y_val, predictions_val)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_val, predictions_val)
r2 = r2_score(y_val, predictions_val)
```

**Интерпретация:**
- **MAE = 0.01** → средняя ошибка ±$1 (в масштабе цен)
- **R² = 0.87** → модель объясняет 87% изменений цены ✓ хорошо
- **RMSE = 0.014** → корень из среднего квадрата ошибок

---

### Блок 6: Визуализация обучения

```python
# Преобразуем Loss в Accuracy для наглядности
train_acc = 1 / (1 + np.array(history['loss']))
val_acc = 1 / (1 + np.array(history['val_loss']))

# График 1: Точность (возрастает)
plt.plot(epochs, train_acc, label='Обучение')      # Синяя линия
plt.plot(epochs, val_acc, label='Валидация')       # Оранжевая линия

# График 2: Потери (убывают)
plt.plot(epochs, history['loss'], label='Обучение')
plt.plot(epochs, history['val_loss'], label='Валидация')
```


## 3. ОТВЕТ НА КОНТРОЛЬНЫЙ ВОПРОС

### Вопрос 3: Что такое теорема Ардена и для чего она используется? 

**Ответ:**

**Теорема Ардена** — математическое правило для решения уравнений с регулярными выражениями вида:  
`X = P + XQ` имеет решение `X = PQ*` (где Q* — ноль или больше повторений Q).

---

### В программировании используется для:

1. **Регулярные выражения** — оптимизация и упрощение сложных паттернов.
2. **Лексические анализаторы** — автоматическая генерация токенизаторов (лексеров) в компиляторах.
3. **Парсеры** — преобразование грамматик в эффективные парсеры (yacc, ANTLR).
4. **Валидация ввода** — преобразование конечных автоматов в регулярные выражения для проверки данных.
5. **Сетевые фильтры / роутеры** — преобразование правил в компактные регулярные выражения.

---

### Простой пример:
Уравнение:  
`X = "a" + X"b"`  
По теореме Ардена:  
`X = "ab*"` (символ `a`, затем ноль или больше `b`).

**На практике:** помогает автоматически превращать описания языков или автоматов в эффективные регулярные выражения для поиска, анализа или фильтрации текста.
